{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized Regressions\n",
    "\n",
    "\n",
    "### The essentials\n",
    "\n",
    "\n",
    "## Regularization\n",
    "\n",
    "Adjustment consists in introducing a penalty term in the cost function (i.e. in this case the sum of squared errors) which allows to control the optimization of the parameters. We therefore have a regularization parameter that must be adjusted in order to obtain a model that gives the best performance on our test (or validation) sample.\n",
    "\n",
    "\n",
    "## Bias vs Variance\n",
    "\n",
    "Whenever we try to model a target variable, we are systematically confronted with a problem: the bias/variance trade-off. The bias consists of a shift between the prediction and the true value of the target variable, the variance represents the instability of the model. A model with too much bias will generalize well (in the sense that the difference between the train error and the test error will be small) but the mean error will be very high in both cases. A model with too high variance will fit exactly to the _train_ data (very small train error) but will not be generalizable to other data (large test error).\n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=1ZX-2sjMUuhI-shVsQXguCLl4yLEp4hfa)\n",
    "\n",
    "\n",
    "## Ridge\n",
    "\n",
    "Ridge is a linear regression model, it allows to predict a quantitative variable, and gives the possibility to control the variance via a regularization parameter. Ridge will tend to penalize significantly high coefficients (greater than 1) and very little low coefficients (less than 1) because it penalizes according to the square of the parameters. The higher the regulation parameter, the closer the coefficients will be to zero.\n",
    "\n",
    "\n",
    "## Lasso\n",
    "\n",
    "Lasso is also a linear regression model. The regularization depends on the absolute value of the parameters, so all the coefficients are penalized in the same way whatever their value. In fact, when some parameters are too unimportant for the prediction of the target variable, the associated coefficient will be reduced to zero. This is how the Lasso model can be used as a variable selection method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
