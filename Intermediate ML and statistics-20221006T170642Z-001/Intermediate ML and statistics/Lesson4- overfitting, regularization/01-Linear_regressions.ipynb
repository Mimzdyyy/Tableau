{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Machine Learning - Linear regression\n",
    "\n",
    "<!-- TOC START min:2 max:4 link:true asterisk:true update:true -->\n",
    "* [Introduction](#introduction)\n",
    "  * [What is the supervised machine learning?](#what-is-the-supervised-machine-learning-)\n",
    "  * [Why do machine learning?](#why-do-machine-learning-)\n",
    "* [Revision linear regressions](#revision-linear-regressions)\n",
    "  * [Simple linear regressions](#simple-linear-regressions)\n",
    "    * [Definition](#definition)\n",
    "    * [Assumptions behind linear regression](#assumptions-behind-linear-regression)\n",
    "    * [Estimate](#estimate)\n",
    "  * [Multiple linear regression](#multiple-linear-regression)\n",
    "    * [Definition](#d√©finition-3)\n",
    "    * [Assumptions behind a multiple linear regression](#assumptions-behind-a-multiple-linear-regression)\n",
    "    * [Final remarks](#final-remarks)\n",
    "<!-- TOC END -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "### What is supervised machine learning?\n",
    "\n",
    "Machine learning is a multidisciplinary science that aims at enabling machines to solve complex problems explicitely coding the decision making process. For example, predicting the conversion rate on a website or the price of real estate are problems that can be solved by optimizing machine learning models.\n",
    "\n",
    "**Supervised machine learning** is a branch of this discipline that consists in solving problems for which there are already solved examples. For instance, data is being collected on a sample of housing units in San Francisco that describe their location, various characteristics and the rent price. If our problem is to estimate the rent price for a dwelling in the same city that is not already in our database, we will build a model from our data that estimates the rent based on the characteristics of the dwellings and apply this model to the unknown dwelling to estimate its rent price. This is a typical example of a supervised learning problem, because when the model was built, the values of the **variable** to be estimated were known, we call it the **target variable**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why should you use machine learning?\n",
    "\n",
    "Once a problem has been indetified, a target variable has been selected, and a number of explanatory variables have been collected, the objectives of supervised statistical modelling fall into three broad, non-exclusive categories:\n",
    "\n",
    "\n",
    "* Description: we can try to understand the relations that exist between the target variable \"Y\" and the explanatory variables \"X1, ..., Xp\" in order to select the most relevant ones or to visualize the behaviors in the observed samples.\n",
    "* Explanation: when you have expert knowledge on the data used for your machine learning problem, for example in economics or biology for example, the objective is to build a test that allows you to verify or confirm theoretical results in practical situations.\n",
    "* Prediction: here we focus on the quality of the estimators and predictors, we do not necessarily try to understand the observed population as well as possible, but to build a model that allows us to produce reliable predictions for future observations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Revision : linear regressions\n",
    "\n",
    "In this section we will focus on **regression models**, that's any machine learning model that allow us to estimate a numerical and **continuous** target variable. We will focus on the concrete example of real estate prices in Boston, using a dataset provided in the scikit learn package from python.\n",
    "\n",
    "\n",
    "### Simple linear regressions\n",
    "\n",
    "#### Definition\n",
    "\n",
    "Simple linear regression models are based on the following linear equation :\n",
    "\n",
    "## $E(Y) = f(X) = \\beta_{0} + \\beta_{1}X_1$\n",
    "\n",
    "#### OR\n",
    "\n",
    "## $Y = \\beta_{0} + \\beta_{1}X_1 + \\epsilon$\n",
    "\n",
    "\n",
    "\n",
    "Here, $Y$ represents the target variable, i.e. the variable whose value we wish to estimate, $X_1$ is the explanatory variable we have chosen to estimate the target variable. $\\beta_0$, $\\beta_{1}$, $\\epsilon$ are respectively the intercept (i.e. level 0 of $Y$ when $X$ is 0), the coefficient associated with $X_1$ (it is the parameter of the model that measures the influence of $X_1$ on $Y$, if $X$ increases by 1, $Y$ will increase by $\\beta_{1}$), and the error or residue of the model. The above equation is the representation of a statistical model: it is true on average but does not claim to be exact, which explains the presence of the residual $\\epsilon$.\n",
    "\n",
    "Depending on the samples in your dataset (see scatter plot below), your model will find the line that comes as close as possible to all the dots on average. Here's what it looks like visually:\n",
    "\n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=1TO3HA0zSs3O2AtNmJSpwuNtqkh5ASGOi)\n",
    "\n",
    "\n",
    "\n",
    "##### Dependent variables\n",
    "\n",
    "In Machine Learning, we always distinguish between the **dependent variable/target variable** and **independent variables/explanatory variables.** The dependent variable is the element you are trying to predict. In the equation above, this corresponds to $Y$.\n",
    "\n",
    "\n",
    "\n",
    "##### Independent Variables\n",
    "\n",
    "The independent variables, represented by $X_i$ ($i$ is an index that indicates the position of the column in a dataset) are your predictors or factors that will be used to determine the value of $Y$. For example, if we try to predict someone's salary based on their years of experience. The independent variable $X_i$ corresponds to the number of years of experience.\n",
    "\n",
    "\n",
    "\n",
    "##### Coefficient\n",
    "\n",
    "The $\\beta_{1}$ coefficient represents the _slope_ or weight your independent variable will have in your equation. Like we mentionned before, the practical interpretation of $\\beta_1$ is the following : if $X_1$ increases by $1$, $Y$ is **expected** to increase by $\\beta_1$. The term **expected** is really important here, remember that any machine learning model is meant to be statistically as good as possible, which means that it is meant to perform the best on average across all given training samples and does not necessarily predicts all individual samples perfectly.\n",
    "\n",
    "\n",
    "\n",
    "##### Constant\n",
    "\n",
    "Finally, the constant $\\beta_{0}$ represents where your line will start if $X = 0$. In the context of predicting wages in relation to years of experience, even at 0 years of experience ($X = 0$), the starting minimum wage will be expected to be different from 0.\n",
    "\n",
    "\n",
    "\n",
    "##### Residual\n",
    "\n",
    "The residual, often noted $\\epsilon$ corresponds to the error committed during modeling. This error corresponds to all the information that is not explained by the model. It is often assumed that the error follows a particular law of probability in order to justify modeling the data with specific kinds of machine learning models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assumptions behind linear regression\n",
    "\n",
    "When building an ML model, you should be aware of the assumptions you are making in order for your model to work well. Otherwise, you will have a poor performance. Remember that traditional machine learning is all based on statistics, which is a sub-section of mathematics, and is an **exact** science, therefore all models that we will study in this module are backed by statistical theory and mathematical proofs that define the theoretical context in which the model can be optimised with certainty. It is very common for these assumptions not to be true in practical contexts, however in most cases, models can still be used, and derive useful results despite all hypothesis being verified. The assumptions needed for a simple linear regression model are the following:\n",
    "\n",
    "\n",
    "### Linearity\n",
    "\n",
    "The first assumption is simple. You need your points to follow roughly a straight line. In other words, you need to ensure that your dependent variable grows linearly as your independent variables increase.\n",
    "\n",
    "\n",
    "\n",
    "### Homoscedasticity\n",
    "\n",
    "Assuming the first assumption is holding and your samples follow a linear dependence between $Y$ and $X_1$, homoscedasticity introduces an assumption on the statistical distribution of the residual $\\epsilon$. Homoscedasticity means that your residuals need to show constant variance accross all possible values of $Y$. If $\\epsilon$ take small values when $Y$ is small and large values when $Y$ is large, then homoscedasticity is not verified.\n",
    "\n",
    "This hypothesis is important in theory in order to mathematically solve the linear regression equation we showed you before, however it does not prevent your model to prove useful in practice.\n",
    "\n",
    "\n",
    "\n",
    "### Independence of residuals\n",
    "\n",
    "The residuals need to be independent from each other. Independence is a very difficult characteristic to verify in practice, therefore it is frequently replaced with absence of autocorrelation (which means correlation with itself). For example if residuals for small values of $Y$ tend to be all negative and residuals for larger values of $Y$ tend to be all positive, that indicates autocorrelation of residuals, which we wish to avoid.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimation\n",
    "\n",
    "Estimation techniques are the methods that we use to calculate the optimal parameters of the model so the residuals are as little as possible accross all data. Here we will introduce two different techniques that can be used to optimize a simple linear regression model, you do not need to know this by heart but rather understand the general logic behind them which roughly goes as follows : we choose a function that needs to be minimized of maximized which is directly linked to the model's equation, this is called the loss function when we need to minimize it, and the likelihood when we need to maximize it. Loss functions usually represent the \"cost\" of prediction errors that wewish to minimize and the likelihood is a probability distribution connecting the target variable to the explanatory variables.\n",
    "\n",
    "### Maximizing Likelihood\n",
    "\n",
    "#### Definition\n",
    "\n",
    "The likelihood of a family of random variables is a function that gives for each possible realisation of each $Œ≤_1$ and $X_{1}$ random variable in the family the probability that this combination of realisations will occur. In the case of statistical modelling, data are already available, so we already know the realizations of each random variable (i.e. the value of the explanatory variables for each observation). It is therefore a matter of finding the parameters that will make the observations available to us as likely as possible. In other words, we are trying to find the underlying general rule that we suspect exists behind the oberved distribution of data. \n",
    "\n",
    "Statistical likelihood is a function of conditional probabilities (it is a probability whose law depends on parameters). Let $X = (X_{1}, X_{2}, ..., X_{n})$, a vector of random variables and $\\theta = (\\beta_{1}, \\beta_{2}, ..., \\beta_{k})$ of the set of parameters on which $X$ depends. In what follows, capital letters represent the random variables i.e. $X_i$ and the lowercase represent secific realisations of thoses random variables $X_i = x_i$. With that in mind, the likelihood of $Y$ is written:\n",
    "\n",
    "\n",
    "### $L(Y)=L(x_{1},x_{2},...x_{p}) = \\prod^{n}_{i=1}f(x_{i};\\beta)$\n",
    "\n",
    "\n",
    "\n",
    "With\n",
    "\n",
    "if x is a continuous variable:\n",
    "\n",
    "### $f(x_{i};\\Beta) = f_{\\beta}(x_{i})$\n",
    "\n",
    "Where $f$ is the density probability function for $X_i$, and $\\Beta$ represents the parameters $\\beta_1, \\beta_2, ..., \\beta_p$.\n",
    "\n",
    "if x is a discret variable:\n",
    "\n",
    "### $f(x_{i};\\Theta) = P_{\\beta}(x_{i})$\n",
    "\n",
    "Where $P$ is the probability of getting $X_i = x_i$.\n",
    "\n",
    "\n",
    "\n",
    "### Maximum likelihood estimate\n",
    "\n",
    "One way to estimate the parameters of a model is to maximize the corresponding likelihood function. In the case of simple linear regression, this function is:\n",
    "\n",
    "### $L(Y = y|\\beta) = \\prod_{i=1}^{n}f(Y_{i} = y_{i}|\\beta)$\n",
    "\n",
    "### $L(Y = y|\\beta) = \\prod_{i=1}^{n}f(\\beta_{0} + \\beta_1 x_1i + \\epsilon = y_{i}|\\beta_{0}, \\beta_{1})$\n",
    "\n",
    "### $L(Y = y|\\beta) = \\prod_{i=1}^{n}f(\\epsilon = y_{i}-\\beta_{0}-\\beta_{1}x_{i}|\\beta_{0}, \\beta_{1}$\n",
    "\n",
    "### $L(Y = y|\\beta) = \\prod_{i=1}^{n}\\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp(\\frac{-(y_{i}-\\beta_{0}-\\beta_{1}x_{i})^2}{2\\sigma^2})$\n",
    "\n",
    "\n",
    "This likelihood equation can be obtained by assuming that the error $\\epsilon$ follows a centred normal law ($E(\\epsilon) = \\mu = 0$) of standard deviation $\\sigma$. We must therefore find the maximum (if it exists) of this likelihood function. To do this we apply a logarithm to each side of the equation to obtain a sum rather than a product.\n",
    "\n",
    "### $log(L(Y = y|\\beta)) = log(\\prod^{n}_{i=1}\\frac{1}{\\sigma \\sqrt{2\\pi}}\\exp(\\frac{-(y_{i}-\\beta_{0}-\\beta_{1}x_{1})^2}{2\\sigma^2}))$\n",
    "\n",
    "### $l(Y = y|\\beta) = -n(log(\\sigma)+log(2\\pi))-\\frac{1}{2\\sigma^2}\\sum^{n}_{i=1}(y_{i}-\\beta_{0}-\\beta_{1}x_{i})^2$\n",
    "\n",
    "\n",
    "\n",
    "We can see that the equation depends only on the parameters of the model: $Œ≤_0$ and $Œ≤_1$ and we still have to find the values for which $-\\sum^{n}_{i=1}(y_{i}-\\beta_{0}-\\beta_{1}x_{i})^2$ is maximal.\n",
    "\n",
    "\n",
    "### The ordinary least square method (commonly referred to as OLS)\n",
    "\n",
    "#### Definition\n",
    "\n",
    "You are probably wondering how we know that the line in our model is the one that is the \"closest\" to each of the points in our dataset. Well, it's because of the \"ordinary least square\" method. We're not going to demonstrate the method all the way. What you have to understand is that the algorithm will look for the minimum possible distance between each point in your graph using this formula:\n",
    "\n",
    "### $Min (\\sum_{i=0}^{n}(y_{i}-\\hat{y}_{i})^2)$\n",
    "\n",
    "We are looking for the set of parameters that will minimize the sum of square errors between the target variable and the prediction. This is actually your first encounter with a **loss function**.\n",
    "\n",
    "### $Min (\\sum_{i=1}^{n}(y_{i}-\\beta{0}-\\beta_{1}x_{i})^2)$\n",
    "\n",
    "In the case of simple linear regression, maximum likelihood or least square estimation is equivalent in terms of finding the optimal set of parameters. However the concept of loss function is becoming more popular, expecially with the rise of deep learning.\n",
    "\n",
    "In this equation, $y_{i}$ represents the value of the target variable $Y$ for each sample (or row) in your dataset while $\\hat{y}_{i}$ represents your model's prediction.\n",
    "\n",
    "After several iterations, your algorithm is able to find the minimize this equation and find the best set of parameters given your training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In python\n",
    "\n",
    "Sklearn is a python library that contains all machine learning models we will use in this course. We import the linear regression model from sklearn using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then create a toy dataset that we will use to try out some sklearn (known as scikit-learn) functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({\n",
    "    \"A\": [1, 2, 3, 4],\n",
    "    \"B\": [12, 23, 34, 36]\n",
    "})\n",
    "\n",
    "X = df[\"A\"].values\n",
    "y = df[\"B\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To optimize the parameters of the model by using the method of least squares:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(X = X.reshape(-1, 1), y= y) # A will serve as our explanatory variable and B is the target variable\n",
    "# we need the reshape(-1,1) method becausewe are using a single explanatory variable, and the fit method\n",
    "# expects X to be a 2 Dimensonal array with rows and columns. Try running this cell without the reshape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make predictions from data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize model and data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n",
       "<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n",
       "<svg height=\"235.34pt\" version=\"1.1\" viewBox=\"0 0 352.7 235.34\" width=\"352.7pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n",
       " <defs>\r\n",
       "  <style type=\"text/css\">\r\n",
       "*{stroke-linecap:butt;stroke-linejoin:round;}\r\n",
       "  </style>\r\n",
       " </defs>\r\n",
       " <g id=\"figure_1\">\r\n",
       "  <g id=\"patch_1\">\r\n",
       "   <path d=\"M 0 235.34 \r\n",
       "L 352.7 235.34 \r\n",
       "L 352.7 0 \r\n",
       "L 0 0 \r\n",
       "z\r\n",
       "\" style=\"fill:none;\"/>\r\n",
       "  </g>\r\n",
       "  <g id=\"axes_1\">\r\n",
       "   <g id=\"patch_2\">\r\n",
       "    <path d=\"M 10.7 224.64 \r\n",
       "L 345.5 224.64 \r\n",
       "L 345.5 7.2 \r\n",
       "L 10.7 7.2 \r\n",
       "z\r\n",
       "\" style=\"fill:#ffffff;\"/>\r\n",
       "   </g>\r\n",
       "   <g id=\"PathCollection_1\">\r\n",
       "    <defs>\r\n",
       "     <path d=\"M 0 3 \r\n",
       "C 0.795609 3 1.55874 2.683901 2.12132 2.12132 \r\n",
       "C 2.683901 1.55874 3 0.795609 3 0 \r\n",
       "C 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \r\n",
       "C 1.55874 -2.683901 0.795609 -3 0 -3 \r\n",
       "C -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \r\n",
       "C -2.683901 -1.55874 -3 -0.795609 -3 0 \r\n",
       "C -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \r\n",
       "C -1.55874 2.683901 -0.795609 3 0 3 \r\n",
       "z\r\n",
       "\" id=\"m9838a8e10c\" style=\"stroke:#000000;\"/>\r\n",
       "    </defs>\r\n",
       "    <g clip-path=\"url(#pe65448c5e3)\">\r\n",
       "     <use style=\"stroke:#000000;\" x=\"26.821874\" xlink:href=\"#m9838a8e10c\" y=\"214.654271\"/>\r\n",
       "     <use style=\"stroke:#000000;\" x=\"127.673958\" xlink:href=\"#m9838a8e10c\" y=\"133.25813\"/>\r\n",
       "     <use style=\"stroke:#000000;\" x=\"228.526042\" xlink:href=\"#m9838a8e10c\" y=\"51.861988\"/>\r\n",
       "     <use style=\"stroke:#000000;\" x=\"329.378126\" xlink:href=\"#m9838a8e10c\" y=\"37.062689\"/>\r\n",
       "    </g>\r\n",
       "   </g>\r\n",
       "   <g id=\"matplotlib.axis_1\"/>\r\n",
       "   <g id=\"matplotlib.axis_2\"/>\r\n",
       "   <g id=\"line2d_1\">\r\n",
       "    <path clip-path=\"url(#pe65448c5e3)\" d=\"M 26.821874 201.334903 \r\n",
       "L 127.673958 139.917814 \r\n",
       "L 228.526042 78.500725 \r\n",
       "L 329.378126 17.083636 \r\n",
       "\" style=\"fill:none;stroke:#0000ff;stroke-linecap:square;stroke-width:3;\"/>\r\n",
       "   </g>\r\n",
       "   <g id=\"patch_3\">\r\n",
       "    <path d=\"M 10.7 224.64 \r\n",
       "L 10.7 7.2 \r\n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n",
       "   </g>\r\n",
       "   <g id=\"patch_4\">\r\n",
       "    <path d=\"M 345.5 224.64 \r\n",
       "L 345.5 7.2 \r\n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n",
       "   </g>\r\n",
       "   <g id=\"patch_5\">\r\n",
       "    <path d=\"M 10.7 224.64 \r\n",
       "L 345.5 224.64 \r\n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n",
       "   </g>\r\n",
       "   <g id=\"patch_6\">\r\n",
       "    <path d=\"M 10.7 7.2 \r\n",
       "L 345.5 7.2 \r\n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n",
       "   </g>\r\n",
       "  </g>\r\n",
       " </g>\r\n",
       " <defs>\r\n",
       "  <clipPath id=\"pe65448c5e3\">\r\n",
       "   <rect height=\"217.44\" width=\"334.8\" x=\"10.7\" y=\"7.2\"/>\r\n",
       "  </clipPath>\r\n",
       " </defs>\r\n",
       "</svg>\r\n"
      ],
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(X, y,  color='black')\n",
    "plt.plot(X, predictions, color='blue', linewidth=3)\n",
    "\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example all assumptions needed to train a simple linear regression model are verified.\n",
    "\n",
    "* There seems to be a linear relation between $Y$ and $X_1$\n",
    "* The deviation between the prediction and the actual values of $Y$, i.e. the residuals, seem to be comparable accross the values of $Y$ (this is Homoscedaticity).\n",
    "* There does not seem to be any autocorrelation going on with the residuals, they seem to be randomly distributed accross all values of $Y$\n",
    "\n",
    "In addition to that, our model seems to be a good predictor based on our training samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple linear regression\n",
    "\n",
    "#### Definition\n",
    "\n",
    "##### Plurality of independent variables\n",
    "\n",
    "Most of the time, you will not want to choose just one variable in order to predict your target variable. For example, you can predict someone's salary with the number of years of experience, but you can also use the type of degree, the sector in which the person works, the gender, the country of residence, and so on.\n",
    "\n",
    "This is the only difference between single and multiple linear regression. You add independent variables into the equation.\n",
    "\n",
    "\n",
    "\n",
    "### Normalizing the variables\n",
    "\n",
    "A major step when preparing your data for prediction is normalization, when should you (or should you not) normalize your data. Well it highly depends on the type of model you are going to use for prediction and the type of analysis you wish to make.\n",
    "\n",
    "Normalization is not needed for multiple linear regression because linear functions treat all variables similarly regardless of scale. A variable like salary described in thousands of euros will have the same effect on the loss function when it varies compared to a variable like age that takes values between 1 and 100 roughly.\n",
    "\n",
    "When you choose not to normalize your data before running a linear regression model, it means that the parameters $\\beta$ in your model can be interpreted directly, meaning if my age variable varies by 1 it means my sample is one year older, then my $Y$ variable, for example salary, will vary by $beta_{age}$. The downside is that the values of the parameters cannot be compared with one another, because each parameters value will depend on the representation scale of the corresponding variable.\n",
    "\n",
    "If you normalize your data the parameters of the model can be compared with one another because all of your variables are represented on the same scale. Then a variation of 1 on $X_{1\\_scaled}$ has an effect of $\\beta_1$ on $Y$ and represents a variation of $\\sigma_{X_1}$ on $X_1$.\n",
    "\n",
    "In python we use the following function for normalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create random features\n",
    "import numpy as np \n",
    "X = np.random.randn(4, 2)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "\n",
    "X = sc.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mathematical notation of the problem\n",
    "\n",
    "By noting $Y$ the target variable, $X_1, X_2, ..., X_n$ the explanatory variables, $\\beta_{i}$ the model parameters and $\\epsilon$ the vector of residuals, the multiple linear regression model is written :\n",
    "\n",
    "### $Y_{i} = \\beta_{0}+X_{i,1}\\beta_{1}+...+X_{i,p}\\beta_{p}+\\epsilon_{i}\\forall i \\in \\left [1,n \\right ]$\n",
    "\n",
    "\n",
    "You can also write the problem in matrix form as follows:\n",
    "\n",
    "### $Y=X \\times \\beta+\\epsilon$\n",
    "\n",
    "\n",
    "Where $Y$ is a vector of $(n, 1)$ dimensions, $X$ is a matrix of $(n, p + 1)$ dimensions, $\\beta$ is a vector of $(p + 1, 1)$ dimensions and $\\epsilon$ is a vector of $(n,1)$ dimensions.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance/Covariance Matrix\n",
    "\n",
    "The variance/covariance matrix of a collection of $p$ random variables indexed by from 1 to $p$ is a square matrix of size $(p, p)$ which elements are :\n",
    "\n",
    "\n",
    "### $\\sigma_{i,j} = Cov(X_{i}, X_j) \\; for \\; i \\in [\\![ 1, p ]\\!] \\; and \\; j \\in [\\![ 1, p ]\\!]$\n",
    "\n",
    "\n",
    "The elements along the diagonal of the variance/covariance matrix are the respective variances of each random variable. The other elements are :\n",
    "\n",
    "\n",
    "### $\\sigma_{ij} = Cov(X_{i}, X_{j}) = \\sum_{i=1}^n (X_{i}-E(X_{i}))(X_{j}-E(X_{j})), i \\neq j$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The covariances between the different random variables.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix resolution by maximum likelihood\n",
    "\n",
    "The calculation of the maximum likelihood estimator is a classic exercise in statistics, so we present the calculation here for those who are familiar with or would like to become familiar with matrix calculation.\n",
    "\n",
    "\n",
    "### $L(Y = y|\\beta) = f(Y=y|\\beta)$\n",
    "\n",
    "### $L(Y = y|\\beta) = f(X\\beta+\\epsilon = y|\\beta)$\n",
    "\n",
    "### $L(Y = y|\\beta) = f(\\epsilon = y-X\\beta|\\beta)$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "According to the assumptions necessary to be able to use a multiple linear model, $\\epsilon$ follows a centered Normal distribution $(E(\\epsilon) = 0*n)$ and diagonal covariance matrix $\\sum = Diag(\\sigma_{1}^2, \\sigma_{2}^{2}, ... , \\sigma_{p}^{2})$. Which brings us to the following equation:\n",
    "\n",
    "### $L(Y = y|\\beta) = det(2\\pi\\sum)^{-\\frac{1}{2}}\\exp(-\\frac{1}{2}((y=X\\beta)^{t}\\sum^{-1}(y-X\\beta)))$\n",
    "\n",
    "\n",
    "$\\sum$ is a diagonal matrix, hence:\n",
    "\n",
    "### $L(Y = y|\\beta) = det(2\\pi\\sum)^{-\\frac{1}{2}}\\exp(-\\frac{1}{2}\\sum^{-1}((y=X\\beta)^{t}(y-X\\beta)))$\n",
    "\n",
    "\n",
    "We apply the logarithm which is increasing and therefore does not change the optimization problem under consideration:\n",
    "\n",
    "### $log(L(Y = y|\\beta)) = -\\frac{1}{2}log(det(2\\pi\\sum))-\\frac{1}{2}\\sum^{-1}(y-X\\beta)^{t}(y-X\\beta)$\n",
    "\n",
    "\n",
    "\n",
    "We try to find the value of $\\beta$ that maximizes the above equation, which is like finding the minimum of the following value:\n",
    "\n",
    "### $\\underset{\\beta}{min}$ $ (y-X\\beta)^{t}(y-X\\beta) = \\beta_{MLE}$\n",
    "\n",
    "### $\\underset{\\beta}{min}$ $ y^{t}y-\\beta^{t}X^{t}y-y^{t}X\\beta+\\beta^{t}X^{t}X\\beta = \\beta_{MLE}$\n",
    "\n",
    "\n",
    "\n",
    "$y^{t}X\\beta$ is a scalar (i.e. a real number of dimension 1), so it is equal to its transpose $y^{t}X\\beta = \\beta^{t}X^{t}y$!, hence:\n",
    "\n",
    "### $\\underset{\\beta}{min}$ $ y^{t}y-2\\beta^{t}X^{t}y + \\beta^{t}X^{t}X\\beta = \\beta_{MLE}$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "We derive from $\\beta$ and we get:\n",
    "\n",
    "### $-X^{t}y + X^{t}X\\beta = 0 \\Rightarrow \\beta_{MLE} = (X^{t}X)^{-1}X^{t}y$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "This solution is well defined only if $X^{t}X$ is an invertible matrix, which is true if the explanatory variables are not collinear and if $p < n$ (the number of explanatory variables is less than the number of observations)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In python, the calculation of the model is done using the following lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression # instanciation of the model\n",
    "model = model.fit(X, y) # fitting the model to data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is indeed the same command as for simple linear regression (because it is the same model!) the only thing that changes is the shape of X!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assumptions for multiple linear regression\n",
    "\n",
    "### Everything in simple linear regression plus NO collinearity\n",
    "\n",
    "As you can already imagine, multiple linear regressions will depend on the same assumptions as simple linear regressions because, after all, you just add a bit of complexity. The only thing you need to add in the assumptions is the _non-colinearity_ of the independent variables.\n",
    "\n",
    "For example, if you are trying to predict someone's salary based on their age and birth year, you will run into a problem. Indeed a linear realtion exists between age and birth year : $age = current\\_year - birth\\_year$.\n",
    "\n",
    "In general, given a collection of variables $X_1, X_2, ..., X_k$, these variable are collinear if and only if you can find a constant $c$ and a collection of constants $\\lambda_1, \\lambda_2, ..., \\lambda_k$ not all equal to zero such that $\\lambda_1 \\times X_1 + \\lambda_2 \\times X_2 + ... + \\lambda_k \\times X_k = c$.\n",
    "\n",
    "If you have collinearity in your model, you won't be able to fit a linear regression model. \n",
    "\n",
    "\n",
    "\n",
    "### Dummy Variables\n",
    "#### Reminder: Categorical variables\n",
    "\n",
    "To understand what dummy variables are, let us first recall what categorical variables are: they are simply qualitative data. For example, think about countries, colors, etc.\n",
    "\n",
    "\n",
    "#### Encode categorical variables\n",
    "\n",
    "In machine learning, you cannot use non numerical variables for training the model. This is why categorical variables are encoded and replaced by a collection of what we call dummy variables that take values of either 0 or 1.\n",
    "\n",
    "\n",
    "\n",
    "#### The dummy variable trap\n",
    "\n",
    "Once you have encoded your dummy variables, you won't add them all into your equation because you will have a collinearity problem between your dummy variables, they all sum to one. So you will add all the dummy variables except one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform simple encoding of categorical variables the following commands can be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Colors_Blue</th>\n",
       "      <th>Colors_Green</th>\n",
       "      <th>Colors_Red</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age  Colors_Blue  Colors_Green  Colors_Red\n",
       "0   10            0             0           1\n",
       "1   12            0             0           1\n",
       "2   32            1             0           0\n",
       "3   21            0             1           0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pd.DataFrame({\n",
    "    \"Colors\": [\"Red\", \"Red\", \"Blue\", \"Green\"],\n",
    "    \"Age\": [10, 12, 32, 21]\n",
    "})\n",
    "\n",
    "pd.get_dummies(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you wish to discard one of the dummy variables you can use a specific argument of the get_dummies function :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Colors_Green</th>\n",
       "      <th>Colors_Red</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age  Colors_Green  Colors_Red\n",
       "0   10             0           1\n",
       "1   12             0           1\n",
       "2   32             0           0\n",
       "3   21             1           0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pd.DataFrame({\n",
    "    \"Colors\": [\"Red\", \"Red\", \"Blue\", \"Green\"],\n",
    "    \"Age\": [10, 12, 32, 21]\n",
    "})\n",
    "\n",
    "pd.get_dummies(X, drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable selection and model selection\n",
    "\n",
    "The main feature of multiple linear regression is that it uses several explanatory variables together. Therefore, the natural question is: which variables should I use to build the best possible model for my objective? This question leads us to introduce model evaluation criteria and variable selection methods.\n",
    "\n",
    "\n",
    "\n",
    "### Evaluation of multiple linear regression models\n",
    "\n",
    "Some of the evaluation criteria presented below may be used for models other than multiple linear regression. It is therefore all the more important to introduce them now and to remember their respective interpretations.\n",
    "\n",
    "\n",
    "\n",
    "#### Analysis of Variance (ANOVA)\n",
    "\n",
    "The analysis of variance allows to quantify the performance of a statistical model in terms of estimation error. The different values that we will discuss now will be used to build other performance metrics:\n",
    "\n",
    "\n",
    "\n",
    "* SST: Sum of Square Total is an indicator of the dispersion of the values of the target variable $Y$ (whose values are noted $y_{1}, ..., y_{n}$) over the population considered, which is written mathematically :\n",
    "\n",
    "### $SST$ $ = \\sum_{i=1}^{n}(y_{i}-\\bar{y})^2$\n",
    "\n",
    "It is the sum of the squared deviations from the mean of the target variable $Y$ for the $n$ observations considered.\n",
    "\n",
    "* SSE: Sum of Square Explained is an indicator that represents the amount of dispersion of the target variable that is explained by the model, which is defined as:\n",
    "\n",
    "### $SSE = \\sum_{i=1}^{n}(\\hat{y}_{i}-\\bar{y})^2$\n",
    "\n",
    "It is the sum of the squared mean differences between the model estimates for each observation and the mean of the target variable for the population of interest. In statistics variation is information, you cannot possibly defferentiate samples if they are all described by the exact same set of values.\n",
    "\n",
    "* SSR: Sum of Squared Residual is an indicator that quantifies the error committed by the model, or in other words the portion of the dispersion of the target variable that is not explained by the model, hence the idea of residual. Its formula is as follows:\n",
    "\n",
    "### $SSR$ $ =\\sum_{i=1}^{n}(y_{i}-\\hat{y}_{i})^2$\n",
    "\n",
    "It is essential to understand these values because they will allow us to build all the evaluation metrics for the multiple linear regression model we will see now.\n",
    "\n",
    "To summarize, SST is proportional to the total variance of the target variable, which can be decomposed into two components : SSE is the variance explained by the model, which is proportional to the amount of variance of our estimates relative to the actual mean of the observed population, and SSR is the sum of the squares of the differences between our estimates and the actual values of the target variable. In other words, SST is the total amount of information, SSE is the information explained by the model and SSR is the information that remains to be explained, or the error committed.\n",
    "\n",
    "\n",
    "\n",
    "#### F-Statistics by Fisher\n",
    "\n",
    "A statistical test is a process by which we try to show whether a hypothesis is confirmed or disproved by the data at our disposal. This test hypothesis, also called null hypothesis and noted $H_{0}$, would have consequences on the properties of the observed data if it is actually verified. These properties are summarized by a test statistic, the value of which gives an idea of the probability that H_0 is true.\n",
    "\n",
    "Fisher's F-statistic allows to test the veracity of the following hypotheses:\n",
    "\n",
    "\n",
    "* When the Fisher test is applied to the model as a whole, the null hypothesis, noted $H_{0}$, is \"the variables chosen to construct the model are not jointly significant in describing the target variable\". If the hypothesis is true, the F-statistic should follow a Fisher probability distribution law noted F-distribution of parameters $(n - 1, n - 1)$ where $n$ is the number of observations used to train the model. However, if the value of the F-statistic, noted \"F\", is outside the most probable regions of the distribution, then we can reject the null hypothesis and conclude that the chosen model has a real explanatory power on the target variable.\n",
    "\n",
    "It may seem a little farfetched but all statistical tests work like that. We make an assumption, this assumption if it held would cause the test statistic to follow a given distribution, if the actual value of the statistic lands too far from the probable scope of the hypothetical distribution we are allowed to reject the null hypothesis.\n",
    "\n",
    "Mathematically, the F-statistic is written:\n",
    "\n",
    "### $F$ $ = \\frac{SSE}{SSR}$\n",
    "\n",
    "The F-test can also compare two nested models (model 1 which includes \"model_1_variables\" and model 2 which includes \"model_1_variables + $X_d$\". In this case the F-statistic follows an F-law of parameters $(n - 1, n - 1)$ if the assumption that the simplest model (model 1) of the two models best describes the target variable is verified. The mathematical formula of F is then :\n",
    "\n",
    "### $F = (\\frac{SSR_{2}-SSR_{1}}{p_{2}-p_{1}})(\\frac{n-p_{1}}{SSR_{1}})$\n",
    "\n",
    "If the value of F-statistic is in an unlikely region of the F-distribution, then the hypothesis is rejected and the test suggests that the more complex Model 2 provides significant additional information compared to the simpler Model 1.\n",
    "\n",
    "Graphically the F-test can be illustrated as follows:\n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=1KKDqtiiimdfHZK59JkZ2wyvDOQUJA8ER)\n",
    "\n",
    "In black, we represent the density distribution of the F-distribution, as in any test we define a level $\\alpha$ between 0 and 1 which will influence the size of the hypothesis rejection zone. Very often, we choose $\\alpha = 5%$ when no specific knowledge can help us modulate our standards. The F test is one-sided, only large values of F will allow us to reject the hypothesis. More precisely if the value of F is in the upper part of the expected distribution equivalent to 5% probability, then we can say that the hypothesis is rejected at $1 - \\alpha$ 95%.\n",
    "\n",
    "This first metric allows us to test the hypothesis that the explanatory variables have no influence on the target variable, we will now look at metrics that indicate the performance level of the model.\n",
    "\n",
    "* $R^{2}$ (R square)\n",
    "\n",
    "$R^{2}$, or R-squared, is a statistic that quantifies the explanatory power of the model with respect to the target variable.\n",
    "\n",
    "### $R^{2} = 1-\\frac{SSR}{SST}$\n",
    "\n",
    "$R^2$ monotonically increases with each additional explanatory variable. It varies between 0 and 1, if the model is not very relevant, the sum of the residual squares $SSR$ will be close to the sum of the total squares $SST$ and $R^{2}$ will be closer to 0, on the contrary, if the model allows to explain the target variable faithfully, then $SSR$ will be closer to 0 and $R^2$ will be closer to 1. So mechanically, with each addition of variable to the model, the prediction of $Y$, the target variable, will be better and $R^2$ will be higher. In fact, $R^2$ is a performance indicator that only allows to compare two models that have the same number of explanatory variables.\n",
    "\n",
    "* $R^2_{adjusted}$\n",
    "\n",
    "$R^2_{adjusted}$ is a modified version of $R¬≤$ that penalizes the number of explanatory variables selected to build the model. Its mathematical formula is:\n",
    "\n",
    "### $R^2_{ajusted} = 1-\\frac{n-1}{n-p-1}(1-R^2)$\n",
    "\n",
    "Where $p$ is the number of explanatory variables used and $n$ is the number of observations used. The growth of $R^2$ as a function of $p$ is compensated by the decrease of $\\frac{n-1}{(n-p-1)}$ as a function of $p$. Consequently, if the information contribution of an explanatory variable is not significant enough, then $R^2_{adjusted}$ will decrease. In fact, it is possible to use this indicator to compare the performance of models that do not necessarily have the same number of explanatory variables.\n",
    "\n",
    "* P-values\n",
    "\n",
    "P-values are evaluation metrics that make it possible to evaluate the contribution of each explanatory variable individually as opposed to evaluating the model as a whole. Unfortunately it cannot be easily computed using sklearn so we will introduce the statsmodels library that calculates all important metrics automatically. The p_value can be interpreted as the probability that a given parameter's true value is 0, in other words the probability that a variable does not bring any significant information to the linear model. Usually we consider that a p_value inferior to 5% means that the variable is significant, otherwise it is consider not significant, however it depends on the context and the standards of the industry you are working in : for example web marketing agencies typically have lower standards than pharmaceutical companies because their goals and constraints are fundamentally different.\n",
    "\n",
    "In order to observe all these indicators in python the following command should be used:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of Square Total 9.74\n",
      "Sum of Square Explained 9.61235365853658\n",
      "Sum of Square Residual 0.1276463414634147\n",
      "\n",
      "\n",
      "R square 0.9868946261331196\n",
      "R square 0.9868946261331196\n",
      "R square adjusted 0.9737892522662392\n",
      "\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "------------------------Results from statsmodels-----------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.987\n",
      "Model:                            OLS   Adj. R-squared:                  0.974\n",
      "Method:                 Least Squares   F-statistic:                     75.30\n",
      "Date:                Wed, 15 Jul 2020   Prob (F-statistic):             0.0131\n",
      "Time:                        15:40:30   Log-Likelihood:                 2.0751\n",
      "No. Observations:                   5   AIC:                             1.850\n",
      "Df Residuals:                       2   BIC:                            0.6781\n",
      "Df Model:                           2                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          4.0554      3.157      1.285      0.328      -9.527      17.638\n",
      "x1             0.5016      0.179      2.798      0.108      -0.270       1.273\n",
      "x2            -0.5512      0.672     -0.820      0.498      -3.442       2.340\n",
      "==============================================================================\n",
      "Omnibus:                          nan   Durbin-Watson:                   2.600\n",
      "Prob(Omnibus):                    nan   Jarque-Bera (JB):                0.614\n",
      "Skew:                          -0.828   Prob(JB):                        0.736\n",
      "Kurtosis:                       2.548   Cond. No.                         169.\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "C:\\Users\\admis\\anaconda3\\lib\\site-packages\\statsmodels\\stats\\stattools.py:71: ValueWarning: omni_normtest is not valid with less than 8 observations; 5 samples were given.\n",
      "  \"samples were given.\" % int(n), ValueWarning)\n"
     ]
    }
   ],
   "source": [
    "# SST, SSE and SSR have to be calculated manually\n",
    "# generate some example data\n",
    "X = np.array([\n",
    "    [1,3,5,6,7],\n",
    "    [4.6, 3.7, 3.4, 3.0, 3.1]\n",
    "]).transpose()\n",
    "Y = np.array([2.1, 3.5, 4.4, 5.6, 5.9])\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression() # create and instanceof the model\n",
    "\n",
    "model.fit(X,Y) # fit the model\n",
    "\n",
    "# calculate evaluation metrics\n",
    "import numpy as np\n",
    "SST = np.sum(np.square(Y - np.mean(Y)))\n",
    "print(\"Sum of Square Total {}\".format(SST))\n",
    "\n",
    "SSE = np.sum(np.square(model.predict(X) - np.mean(Y)))\n",
    "print(\"Sum of Square Explained {}\".format(SSE))\n",
    "\n",
    "SSR = np.sum(np.square(Y - model.predict(X)))\n",
    "print(\"Sum of Square Residual {}\".format(SSR))\n",
    "print(\"\\n\")\n",
    "\n",
    "# calculate R square and adjusted R-square\n",
    "R_2 = 1 - SSR/SST\n",
    "print(\"R square {}\".format(R_2))\n",
    "R_2_alt = model.score(X,Y) # alternative method to calculate R square\n",
    "print(\"R square {}\".format(R_2_alt))\n",
    "n = X.shape[0]\n",
    "p = X.shape[1]\n",
    "R_2_adj = 1 - (n-1)/(n-p-1)*(1-R_2)\n",
    "print(\"R square adjusted {}\".format(R_2_adj))\n",
    "\n",
    "# alternative solution with library statsmodels (useful mainly for linear models)\n",
    "import statsmodels.api as sm\n",
    "\n",
    "X2 = sm.add_constant(X) # the coefficient beta_0 also called intercept is not automatically included, so we need to manually add a constant variable equal to one.\n",
    "est = sm.OLS(Y, X2)\n",
    "est2 = est.fit()\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------------------------------------------------------------------------\")\n",
    "print(\"------------------------Results from statsmodels-----------------------------------------\")\n",
    "print(\"-----------------------------------------------------------------------------------------\")\n",
    "print(\"\\n\")\n",
    "print(est2.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection.\n",
    "\n",
    "When we have at our disposal $p$ explanatory variables, the number of models that it is possible to construct can be counted in the following way: for each explanatory variable we can build a model with or without it, applying this reasonning to all explanatory variables we can potentially build $2^p$ models. In practice, when the number of $p$ explanatory variables is large, we cannot reasonnably explore the $2^p$ models that can be built in order to select the best one. Different methods exist which allow to avoid using brute force.\n",
    "\n",
    "### Step by step\n",
    "\n",
    "The step-by-step selection is divided into three variants:\n",
    "\n",
    "* Forward selection: the variables are added one by one to the model by selecting at each step the one that maximises the F statistic. It stops when all the variables are used or when the null hypothesis of increase in explained information cannot be rejected, i.e. when $F = (\\frac{SSR_{2}-SSR_{1}}{p_{2}-p_{1}})(\\frac{n-p_{1}}{SSR_{1}})$ falls into the probable area of the F-distribution.\n",
    "* Elimination (backward): This time we start with a model using all the explanatory variables. At each step, the variable with the highest p-value associated with the Fisher test is eliminated from the model. The procedure stops when all the remaining variables have p-values higher than a threshold set by default at 0.05 (but which can be adapted according to the precision needs of the considered problem).\n",
    "* Stepwise: This algorithm alternates between a selection step and an elimination step after each addition of a variable, in order to remove any variables that would have become less relevant in the presence of those that have been added. Example, if after adding the most useful variable according to the F-stat criterion one of the variables becomes non-significant, we remove it and proceed with a new forward selection step.\n",
    "\n",
    "### Final remarks\n",
    "\n",
    "Linear models are very sensitive to extreme values that may be present in a dataset, so pre-processing your learning base is essential to avoid having your results completely skewed.\n",
    "\n",
    "The model evaluation and selection methods introduced above are perfectly valid for all linear models, as well as the logistic regression that we will discuss later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
